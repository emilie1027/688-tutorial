{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer - A Deep Learning Approach\n",
    "\n",
    "## Why use deep learning?\n",
    "\n",
    "Deep learning has substantially pushed the state-of-the-art in a wide range of tasks, including speech recognition, image recognition, and object detection. In 2012, deep neural networks shocked the world by halving the error rate in the ImageNet contest, and precipitated the rapid adoption of deep learning by the whole community. From then on, practitioners just don't get any result if they don't use deep learning. Therefore, let's take a look at this hottest technique.\n",
    "\n",
    "\n",
    "## Why recognize digits?\n",
    "\n",
    "Recognizing digits with MNIST dataset is the hello world example in the machine learning world. Specifically, it is simple, but illustrative enough. For those who take 688, you must have got familiar with it during lab4, and we reuse some code from lab4 to maintain the consistency. For those who don't, The MNIST database is a large database of handwritten digits (0-9) that contains 60,000 training images and 10,000 testing images. Each image is a 28Ã—28 pixel image. Below is the code for loading the database into numpy array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def parse_images(filename):\n",
    "    f = open(filename,\"rb\");\n",
    "    magic,size = struct.unpack('>ii', f.read(8))\n",
    "    sx,sy = struct.unpack('>ii', f.read(8))\n",
    "    X = []\n",
    "    for i in range(size):\n",
    "        im =  struct.unpack('B'*(sx*sy), f.read(sx*sy))\n",
    "        X.append([float(x)/255.0 for x in im]);\n",
    "    return np.array(X).transpose();\n",
    "\n",
    "def parse_labels(filename):\n",
    "    f = open(filename,\"rb\");\n",
    "    magic,size = struct.unpack('>ii', f.read(8))\n",
    "    return np.array(struct.unpack('B'*size, f.read(size)))\n",
    "\n",
    "# dataset can be downloaded from http://yann.lecun.com/exdb/mnist/\n",
    "train_images = parse_images(\"train-images-idx3-ubyte\")\n",
    "train_labels = parse_labels(\"train-labels-idx1-ubyte\")\n",
    "test_images = parse_images(\"t10k-images-idx3-ubyte\")\n",
    "test_labels = parse_labels(\"t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's have a look at some samplings of the digits, such that we have a feeling of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "M,N = 10,20\n",
    "fig, ax = plt.subplots(figsize=(N,M))\n",
    "digits = np.vstack([np.hstack([np.reshape(train_images[:, i*N+j],(28,28)) \n",
    "                               for j in range(N)]) for i in range(M)])\n",
    "ax.imshow(255-digits, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![MNIST.png](https://s11.postimg.org/azqm5mg9f/MNIST.png)](https://postimg.org/image/8v694jemn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n",
    "\n",
    "Deep, in its appearance, means that the networks have great depth, i.e. have many layers of hidden units. 20 years ago a typical neural network had 3-5 layers of hidden units, while recently Microsoft researchers won the ImageNet contest with 152 layers neural networks!\n",
    "\n",
    "However, deep learning is not just stacking layer over layer. Without modern architectures and techniques, a deeper network, with hundreds of millions of parameters, may suffer from (to name a few):\n",
    "1. Hard to train.\n",
    "2. Overfitting.\n",
    "\n",
    "To solve these issues, scientists introduced new architectures, like convolutional networks, and new training techniques, like dropout. Therefore, Deep Neural Networks (DNNs), in essence, are neural networks with modern architectures, and are trained with modern techniques. In this tutorial, we will first introduce a simple neural network, and then proceed to some cool deep techniques.\n",
    "\n",
    "## Multiclass classification and one-hot representation\n",
    "\n",
    "Classification of the digits is a multiclass (10 classes) classification. In binary classification problem, we use 0 to represent one class, while use 1 to represent another. This can be easily extended to the multiclass scenario. Specifically, assuming we have $C$ classes, we use a binary vector of length C, all except one of which are zeros, and thus it is called one-hot. Say the original label is k, then only the kth element of the one-hot vector is 1. Below is the code for converting dense labels to one-hot labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return np.transpose(labels_one_hot)\n",
    "\n",
    "train_labels = dense_to_one_hot(train_labels)\n",
    "test_labels = dense_to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Neural networks\n",
    "\n",
    "Neural networks are made of a collection of neurons, and each neuron takes multiple inputs and then outputs usually a binary value. As in the lab 3, we add a bias $b$ to the input, and the weight $w_0$ for bia is usually set as 1. Below shows a single neuron. Mathematically, the output $y$ satisfies the following formula:\n",
    "\n",
    "$$y = \\sigma(w_1x_1 + w_2x_2 + b)$$\n",
    "\n",
    "where $\\sigma(x)$ is the sigmoid function, and is defined as $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$. The sigmoid is also called activation function, which limits the output between 0 and 1. If $y > 0.5$, the neuron is activated; otherwise, it is deactivated. \n",
    "\n",
    "[![single_neuron.png](https://s22.postimg.org/kvfm6qhc1/single_neuron.png)](https://postimg.org/image/p4kc8wkl9/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x)) # compute sigmoid in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful ability of neural networks is that they can, rather than need to be fed with manually crafted features, directly learn abstract representations from the raw input. Therefore, all we need to do is to construct a network and throw all inputs to it. In our case, we have 28 $\\times$ 28 pixel images, and thus the size of input is 784. The output size is 10, as we have 10 classes in one-hot representation. Also, we need to have some neurons in between input and output to model the underlying mapping between pixels and classes. As a first attempt, let's use one layer of neurons and set the size of neurons as 256. (You may ask why we choose 256. The answer is that it does not matter once it is within a range, and you can choose whatever you like. 256 is a power of 2.)\n",
    "\n",
    "For simplicity of mathematical notation, we introduce the matrix formulation of the previous equation. Now, the input $\\mathbf{x}$ is a vector of 784 elements, the weight matrix $\\mathbf{W_1}$ is 256 $\\times$ 784, $\\mathbf{W_2}$ is 10 $\\times$ 256, and the output $\\mathbf{y}$ is a vector of 10 elements.\n",
    "\n",
    "$$h(\\mathbf{x}) = \\sigma(\\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1})$$\n",
    "$$\\mathbf{y} = \\sigma(\\mathbf{W_2}h(\\mathbf{x}) + \\mathbf{b_2})$$\n",
    "\n",
    "Finally, we should classify the image as the digit we think is most probable. Hence, we should use maximum. However, we instead use soft maximum function here, as it is easier to compute its gradient, which will benefit our stochastic gradient descent optimization later. The softmax function is defined as:\n",
    "\n",
    "$$ p(y = i) = exp(y_i)/\\sum_{j=0}^{C}{exp(y_j)}$$\n",
    "\n",
    "Intuitively, the softmax function outputs a number between 0 and 1, which can be viewed as the probability of the image belonging to a certain class. For better understanding, the architecture of the network is illustrated in the following graph (bias is omitted).\n",
    "\n",
    "[![one_layer_network.png](https://s22.postimg.org/4yt40k4ap/one_layer_network.png)](https://postimg.org/image/uucujr64d/)\n",
    "\n",
    "Also, the corresponding piece of code is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp_x = np.sum(exp_x, 0)\n",
    "    return np.divide(exp_x, sum_exp_x)\n",
    "\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    h1 = sigmoid(np.add(np.matmul(W1, x), b1))\n",
    "    y = sigmoid(np.add(np.matmul(W2, h1), b2))\n",
    "    p = softmax(y)\n",
    "    return h1, y, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have officially finished the so-called feed-forward process of neural networks. Now, we have many adjustable parameters, $\\mathbf{W_1}$, $\\mathbf{W_2}$, $\\mathbf{b_1}$, and $\\mathbf{b_2}$, it is time to learn these parameters. \n",
    "\n",
    "Image that the network outputs a vector (0.1, 0.1, ..., 0.1) for a certain image, while the label for this image is 0. Thus, the one-hot representation is (1, 0, 0, ..., 0). Certainly, we want the network's output to match the one-hot representation, i.e. increase the probability of outputting 0, and decrease the probability of other digits. Thus, we want to change the output $\\mathbf{y}$ by ($\\Delta y$, $-\\Delta y$, $-\\Delta y$, ..., $-\\Delta y$), where $\\Delta y$ is a small positive number, by changing individual parameters, say $b_1$. Apparently, this is where gradient should come into the scene. Specifically, the new $b_1$ should be set as something like:\n",
    "\n",
    "$$b_1 = b_1 + \\alpha \\frac{\\partial{\\Delta \\mathbf{y}}}{\\partial{b_1}}$$\n",
    "\n",
    "where $\\alpha$ is a hyper-parameter we should decide later. Also, $\\alpha$ controls how far each step we take, and thus is known as the learning rate. \n",
    "\n",
    "Until now the $\\Delta y$ part is just intuition, we want to formalize it mathematically. As we mentioned before, the output of softmax function is the probability that one image belongs to a certain class. Therefore, we can construct a MLE estimator, and we hope to maximize the likelihood. Given a certain image, we obtain:\n",
    "\n",
    "$$ p_{MLE} = \\prod_{i=0}^{C}{p_i}^{t_i} $$\n",
    "\n",
    "where $t_k = 1$ if $k$ is the true label. Note that when $t_i = 0$, ${p_i}^{t_i}$ gives a one, and when $t_i = 1$, it gives $p_i$. Thus, this is equivalent to maximize $p_k$, where $k$ is the correct label. However, this mathematical notation makes the manipulation simpler. Moreover, equivalently, it is usually more convenient to minimize the negative log-likelihood, and we call it loss function. Also, this famous loss function is named as cross-entropy loss.\n",
    "\n",
    "$$ L = -\\sum_{i=0}^{C}{t_i}\\log{p_i} $$\n",
    "\n",
    "Therefore, instead of using $\\Delta \\mathbf{y}$, formally, we have the update rule as:\n",
    "\n",
    "$$\\mathbf{W} = \\mathbf{W} - \\alpha \\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$$\n",
    "$$\\mathbf{b} = \\mathbf{b} - \\alpha \\frac{\\partial{L}}{\\partial{\\mathbf{b}}}$$\n",
    "\n",
    "Since $L$ is a function of $\\mathbf{p}$ and $\\mathbf{p}$ is function of $\\mathbf{y}$, we denote as $L = o(\\mathbf{y})$. According to the chain rule in calculus, it is easy to get \n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{W_2}}} = \n",
    "\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\n",
    "\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W_2}}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{b_2}}} = \n",
    "\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\n",
    "\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{b_2}}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{W_1}}} = \n",
    "\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\n",
    "\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}}\n",
    "\\frac{\\partial{h(\\mathbf{x})}}{\\partial{\\mathbf{W_1}}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{b_1}}} = \n",
    "\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\n",
    "\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}}\n",
    "\\frac{\\partial{h(\\mathbf{x})}}{\\partial{\\mathbf{b_1}}}$$\n",
    "\n",
    "Think of this procedure: we first have the loss w.r.t \\mathbf{y}, which is the term $\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}$. Then, based on this term, we can calculate the loss w.r.t. $\\mathbf{W_2}$ and $\\mathbf{b_2}$ as well as $h(\\mathbf{x})$, which is $\\frac{\\partial{L}}{\\partial{\\mathbf{y}}}\n",
    "\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}}$. Finally, based on the loss w.r.t. $h(\\mathbf{x})$, we can compute the loss w.r.t. $\\mathbf{W_1}$ and $\\mathbf{b_1}$. The process looks like we are propagating the loss from $L$ to $\\mathbf{y}$ to $h(\\mathbf{x})$ and finally to every parameter, and this is exactly why this learning process is named back-propagation.\n",
    "\n",
    "With simple calculus and some mathematical manipulation, we have get the value of each component for the above rules. To save some space, here list some important results as an example: \n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{y}}} = \\mathbf{p} - \\mathbf{t}$$\n",
    "$$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{b_1}}} = \\mathbf{y}(1-\\mathbf{y})$$\n",
    "$$\\frac{\\partial{\\mathbf{y}}}{\\partial{h(\\mathbf{x})}} = \\mathbf{W_2}^T\\mathbf{y}(1-\\mathbf{y})$$\n",
    "\n",
    "Interestingly, the loss w.r.t. y is $\\mathbf{p} - \\mathbf{t}$, which exactly matches our intuition. Thus, we can actually view loss functions as a mathematically formulation of our intuition.\n",
    "\n",
    "All of above is the famous optimization technique gradient descent. One thing left is that we need to repeatedly apply these update rules until the loss function reaches a local minimum (Optimization of neural networks is non-convex, and this is also part of the reason why we need to use gradient descent). Basically, the loss on the training set will keep going down, but we actually desire to have a high accuracy on the test set. They are not equivalent. One thing is overfitting. Generally, the loss on the test set will decrease first and then after some points begin to increase. Therefore, we hope to stop before the loss on the test set go up. A common approach is to specify a maximum amount of steps for taking gradient, which can be selected based on cross-validation. \n",
    "\n",
    "Before we proceed to training, we first need to figure out some ways to evaluate our approach. Also, this acts like a debugging technique to make sure our code really works. Basically, we have four ways: the loss and error rate on the training set, and the loss and error on the test set. Specifically, during training, we are pretty sure that the loss function on the training must decrease, and the error rate should have a decreasing trend. For the test set, we need to evaluate the result after training, and see how well our result can generalize.\n",
    "\n",
    "The evaluation code is as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, truth):\n",
    "    entropy = -np.add(np.multiply(truth, np.log(y)), \\\n",
    "                  np.multiply(-np.subtract(truth, 1), np.log(-np.subtract(y, 1))))\n",
    "    mean_entropy = np.sum(entropy) / entropy.shape[1]\n",
    "    return mean_entropy\n",
    "\n",
    "def error_rate(y, truth):\n",
    "    return np.mean(np.argmax(truth, 0) != np.argmax(y, 0))\n",
    "\n",
    "def evaluate(y, truth):\n",
    "    loss = cross_entropy(p, truth)\n",
    "    error = error_rate(p, truth)\n",
    "    return loss, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start our training code. Note that, instead of computing gradient for each example and summing them, we concatenate the examples together, such that $\\mathbf{x}$ becomes 784 $\\times$ 60,000, and we can use matrix multiplication, which is much faster. For now, we have two hyper-parameters, learning rate and epochs, and we, somewhat randomly, set them as 1e-4 and 100 in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weight_variables(dim1, dim2, variance):\n",
    "    matrix = np.multiply(2, np.random.rand(dim1, dim2))\n",
    "    matrix = np.multiply(np.subtract(matrix, 1), variance)\n",
    "    return matrix\n",
    "\n",
    "def bias_variables(dim1, dim2, variance):\n",
    "    matrix = np.ones((dim1, dim2))\n",
    "    matrix = np.multiply(matrix, variance)\n",
    "    return matrix\n",
    "\n",
    "import math\n",
    "\n",
    "num_hidden_units = 256\n",
    "learn_rate = 1e-4\n",
    "epochs = 200\n",
    "\n",
    "images = train_images\n",
    "labels = train_labels\n",
    "\n",
    "variance1 = math.sqrt(6.0) / (784.0 + num_hidden_units)\n",
    "W1 = weight_variables(num_hidden_units, 784, variance1)\n",
    "b1 = bias_variables(num_hidden_units, 1, 0)\n",
    "\n",
    "variance2 = math.sqrt(6.0) / (10.0 + num_hidden_units)\n",
    "W2 = weight_variables(10, num_hidden_units, variance2)\n",
    "b2 = bias_variables(10, 1, 0)\n",
    "\n",
    "images_transposed = np.transpose(images)\n",
    "images_ones = np.ones((images_transposed.shape[0], 1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    h1, y, p = feed_forward(images, W1, b1, W2, b2)\n",
    "\n",
    "    # every 20 steps print train loss and error rate\n",
    "    if epoch % 20 == 0:\n",
    "        train_loss, train_error = evaluate(p, train_labels)\n",
    "        print \"training loss\", train_loss, \"training error:\", train_error\n",
    "    \n",
    "    h1_tranposed = np.transpose(h1)\n",
    "    \n",
    "    loss_derivative = np.subtract(p, labels)\n",
    "    output = np.multiply(loss_derivative, np.multiply(y, (1-y)))\n",
    "    hidden = np.multiply(np.matmul(np.transpose(W2), output), np.multiply(h1, (1-h1)))\n",
    "\n",
    "    W2_gradient = np.matmul(output, h1_tranposed)\n",
    "    b2_gradient = np.matmul(output, np.ones((h1_tranposed.shape[0], 1)))\n",
    "    W2 = np.subtract(W2, np.multiply(learn_rate, W2_gradient))\n",
    "    b2 = np.subtract(b2, np.multiply(learn_rate, b2_gradient))\n",
    "\n",
    "    W1_gradient = np.matmul(hidden, images_transposed)\n",
    "    b1_gradient = np.matmul(hidden, images_ones)\n",
    "    W1 = np.subtract(W1, np.multiply(learn_rate, W1_gradient))\n",
    "    b1 = np.subtract(b1, np.multiply(learn_rate, b1_gradient))\n",
    "\n",
    "_, _, p = feed_forward(train_images, W1, b1, W2, b2)\n",
    "train_loss, train_error = evaluate(p, train_labels)\n",
    "print \"training loss\", train_loss, \"training error:\", train_error\n",
    "_, _, p = feed_forward(test_images, W1, b1, W2, b2)\n",
    "test_loss, test_error = evaluate(p, test_labels)\n",
    "print \"test loss\", test_loss, \"test error:\", test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "training loss 3.25064678292 training error: 0.887633333333\n",
    "training loss 3.20428458326 training error: 0.846083333333\n",
    "training loss 2.76691405222 training error: 0.325966666667\n",
    "training loss 2.63884964838 training error: 0.309883333333\n",
    "training loss 2.56982048941 training error: 0.2524\n",
    "training loss 2.52559156841 training error: 0.2132\n",
    "training loss 2.49819515957 training error: 0.188183333333\n",
    "training loss 2.47981450903 training error: 0.1759\n",
    "training loss 2.46614326558 training error: 0.16835\n",
    "training loss 2.45483191902 training error: 0.1654\n",
    "training loss 2.44510834604 training error: 0.16915\n",
    "test loss 2.43895755975 test error: 0.166\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, you should see something similar to above. Actually, if you keep running, the error rate probably will keep decreasing, but the code is too slow, and we will introduce better techniques in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Going deeper\n",
    "\n",
    "Actually, the above is just the simplest neural network, and the final result is pretty crappy in neural networks' world. To make it better, we will introduce several more advanced techniques, stochastic gradient descent, initialization, weight decay, momentum, and dropout.\n",
    "\n",
    "### Stochastic gradient descent\n",
    "\n",
    "Gradient descent calculates perfect gradient of the training set, but the gradient may not model the test set well. Stochastic gradient descent (SGD), as its name suggests, introduces some stochasticity to the model, which potentially lessens the overfitting. Here, we use a mini-batch SGD, every time we use a mini-batch of size $N$ to calculate the gradient. This can utilize the matrix multiplication, which is one of the most optimized code in the world, and thus the computation is much more efficient. Also, mini-batch SGD gives less stochasticity than SGD, but more than gradient descent. This strikes a balance between the two method, which is proven to be very effective in practice. Below is the code to get a random mini-batch from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size=50):\n",
    "    indexes = np.random.randint(0, 60000, batch_size)\n",
    "    return train_images[:, indexes], train_labels[:, indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Sigmoid function is a S-shape function, and has a dynamic range. Typicall, the dynamic range is set as $[-\\sqrt{3}, \\sqrt{3}]$. Outside the range, the neuron may always output 1 or 0, which causes the loss of capacity. The initialization is illustrated as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(8,1.5))\n",
    "x = np.arange(-4,4,0.05)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "ones = np.ones((11, 1))\n",
    "straight = np.arange(0,1.1,0.1)\n",
    "plt.plot(ones * np.sqrt(3), straight, 'r--')\n",
    "plt.plot(ones * -np.sqrt(3), straight, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![sigmoid_range.png](https://s22.postimg.org/3xk53bx75/sigmoid_range.png)](https://postimg.org/image/5clps1ya5/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay and maximum norm\n",
    "\n",
    "Similar reasons above, we don't want the weight to become too large, and thus we want to limit the absolute value of the weight. Besides, a very large weight usually leads to more overfitting. The first way is to add a weight decay term to the loss function, i.e. \n",
    "\n",
    "$$ L = -\\sum_{i=0}^{C}{t_i}\\log{p_i} + \\lambda\\sum{\\mathbf{W}^2}$$\n",
    "\n",
    "where $\\lambda$ controls the extent of weight decay. From this loss function, we know that we want to push the weights towards zero. Note that, formally, weight decay can be modelled as a MAP estimator, where the weight decay is a Gaussian prior. Specifically,\n",
    "\n",
    "$$ p_{MAP} = p(\\mathbf{W})\\prod_{i=0}^{C}{p_i}^{t_i} $$\n",
    "\n",
    "where $p(\\mathbf{W})$ is a zero mean Gaussian distribution. The second way is to constraint the maximum value directly, e.g. cap the absolute value of weights to C, i.e. $||\\mathbf{W}|| \\leq 2$. A justification is that constraining weight vectors to lie inside a ball of fixed radius makes it possible to use a huge learning rate without the possibility of weights blowing up. The following is code to limit maximum norm of weights as $C$, and we will show the weight decay when we calculate the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm(W, C):\n",
    "    W[W > C] = C\n",
    "    W[W < -C] = -C\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "As mentioned before, optimization of neural networks is a non-convex problem. Therefore, it is prone to get stuck in local optima. The momentum solve this problem by the idea that we can escape the local optimum by a push if it is a poor and shallow optimum, and momentum is just that push. Instead of using only current gradient, we also take into account the gradient in the previous steps. Formally, the gradient considering momentum $\\hat{\\Delta}^{(t)}$ is \n",
    "\n",
    "$$ \\hat{\\Delta}^{(t)} = \\Delta^{(t)} + \\gamma \\hat{\\Delta}^{(t - 1)} $$\n",
    "\n",
    "where $\\gamma$ is the strength of momentum and $\\Delta^{(t)}$ is the current gradient of loss function $L$.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Another shortcoming of current back-propagation algorithm is that all neurons are updated together. One way to break the symmetry is that we use random initialization. Another is the dropout. The idea is that, during training, we randomly disable some neurons. Specifically, we treat those disabled neurons as non-existent, and some steps remain exact the same. Therefore, instead of training one neural network, we are actually training a bunch of neural networks, and a subset of neurons should be able to independently predict the result. Finally, in testing, we multiply the output of all neurons with a constant (1 - drop_rate) to offset the dropout in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_mask(dim, dropout, train=True):\n",
    "    # the mask for dropout during train and test, respectively\n",
    "    if train:\n",
    "        mask = np.random.rand(dim, 1)\n",
    "        mask[mask < dropout] = 0\n",
    "        mask[mask >= dropout] = 1\n",
    "    else:\n",
    "        mask = (1 - dropout) * np.ones((dim, 1)) \n",
    "    return mask\n",
    "\n",
    "def feed_forward_with_dropout(x, W1, b1, W2, b2, W3, b3, dropout, dim, train=True):\n",
    "    mask = gen_mask(dim, dropout, train)\n",
    "    h1 = np.multiply(mask, sigmoid(np.add(np.matmul(W1, x), b1)))\n",
    "    h2 = np.multiply(mask, sigmoid(np.add(np.matmul(W2, h1), b2)))\n",
    "    y = sigmoid(np.add(np.matmul(W3, h2), b3))\n",
    "    p = softmax(y)\n",
    "    return h1, h2, y, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these advanced methods in hand, we can run a much larger network. Now, let's increase hidden layers to 2 layers with 256 neurons per layer. The hyper-parameters are chosen based on our experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(15388) # fix the seed\n",
    "\n",
    "num_hidden_units1 = 256\n",
    "num_hidden_units2 = 256\n",
    "\n",
    "variance1 = math.sqrt(6.0) / (784.0 + num_hidden_units1)\n",
    "W1 = weight_variables(num_hidden_units1, 784, variance1)\n",
    "b1 = bias_variables(num_hidden_units1, 1, variance1)\n",
    "\n",
    "variance2 = math.sqrt(6.0) / (num_hidden_units1 + num_hidden_units2)\n",
    "W2 = weight_variables(num_hidden_units2, num_hidden_units1, variance2)\n",
    "b2 = bias_variables(num_hidden_units2, 1, variance2)\n",
    "\n",
    "variance3 = math.sqrt(6.0) / (10.0 + num_hidden_units2)\n",
    "W3 = weight_variables(10, num_hidden_units2, variance3)\n",
    "b3 = bias_variables(10, 1, variance3)\n",
    "\n",
    "learn_rate = 0.01\n",
    "steps = 60000 # train on one mini-batch is one step\n",
    "momentum = 0.95\n",
    "dropout = 0.5\n",
    "weight_decay = 0.0\n",
    "C = 2\n",
    "\n",
    "last_W3 = np.zeros(W3.shape)\n",
    "last_b3 = np.zeros(b3.shape)\n",
    "last_W2 = np.zeros(W2.shape)\n",
    "last_b2 = np.zeros(b2.shape)\n",
    "last_W1 = np.zeros(W1.shape)\n",
    "last_b1 = np.zeros(b1.shape)\n",
    "\n",
    "for step in range(0, steps):\n",
    "    images, labels = get_batch()\n",
    "    h1, h2, y, p = feed_forward_with_dropout(\n",
    "        images, W1, b1, W2, b2, W3, b3, dropout, num_hidden_units1)\n",
    "\n",
    "    images_transposed = np.transpose(images)\n",
    "    images_ones = np.ones((images_transposed.shape[0], 1))\n",
    "    \n",
    "    # every 5000 steps print train loss and error rate\n",
    "    if step % 20000 == 0:\n",
    "        train_loss, train_error = evaluate(p, labels)\n",
    "        print \"step\", step, \"training loss\", train_loss, \"training error:\", train_error\n",
    "    \n",
    "    h1_tranposed = np.transpose(h1)\n",
    "    h2_tranposed = np.transpose(h2)\n",
    "    \n",
    "    loss_derivative = np.subtract(p, labels)\n",
    "    output = np.multiply(loss_derivative, np.multiply(y, (1-y)))\n",
    "    hidden2 = np.multiply(np.matmul(np.transpose(W3), output), np.multiply(h2, (1-h2)))\n",
    "    hidden1 = np.multiply(np.matmul(np.transpose(W2), hidden2), np.multiply(h1, (1-h1)))\n",
    "\n",
    "    W3_gradient = np.matmul(output, h2_tranposed) \\\n",
    "        + (momentum * last_W3) + weight_decay * W3\n",
    "    b3_gradient = np.matmul(output, np.ones((h2_tranposed.shape[0], 1))) \\\n",
    "        + (momentum * last_b3)\n",
    "    W3 = np.subtract(W3, np.multiply(learn_rate, W3_gradient))\n",
    "    b3 = np.subtract(b3, np.multiply(learn_rate, b3_gradient))\n",
    "\n",
    "    W2_gradient = np.matmul(hidden2, h1_tranposed) \\\n",
    "        + (momentum * last_W2) + weight_decay * W2\n",
    "    b2_gradient = np.matmul(hidden2, np.ones((h1_tranposed.shape[0], 1))) \\\n",
    "        + (momentum * last_b2)\n",
    "    W2 = np.subtract(W2, np.multiply(learn_rate, W2_gradient))\n",
    "    b2 = np.subtract(b2, np.multiply(learn_rate, b2_gradient))\n",
    "\n",
    "    W1_gradient = np.matmul(hidden1, images_transposed) \\\n",
    "        + (momentum * last_W1) + weight_decay * W1\n",
    "    b1_gradient = np.matmul(hidden1, images_ones) \\\n",
    "        + (momentum * last_b1)\n",
    "    W1 = np.subtract(W1, np.multiply(learn_rate, W1_gradient))\n",
    "    b1 = np.subtract(b1, np.multiply(learn_rate, b1_gradient))\n",
    "    \n",
    "    W3 = max_norm(W3, C)\n",
    "    W2 = max_norm(W2, C)\n",
    "    W1 = max_norm(W1, C)\n",
    "    \n",
    "    last_W3 = W3_gradient\n",
    "    last_b3 = b3_gradient\n",
    "    last_W2 = W2_gradient\n",
    "    last_b2 = b2_gradient\n",
    "    last_W1 = W1_gradient\n",
    "    last_b1 = b1_gradient\n",
    "    \n",
    "_, _, _, p = feed_forward_with_dropout(\n",
    "    train_images, W1, b1, W2, b2, W3, b3, dropout, num_hidden_units1, False)\n",
    "train_loss, train_error = evaluate(p, train_labels)\n",
    "print \"training loss\", train_loss, \"training error:\", train_error\n",
    "_, _, _, p = feed_forward_with_dropout(\n",
    "    test_images, W1, b1, W2, b2, W3, b3, dropout, num_hidden_units1, False)\n",
    "test_loss, test_error = evaluate(p, test_labels)\n",
    "print \"test loss\", test_loss, \"test error:\", test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "step 0 training loss 3.24968681136 training error: 0.86\n",
    "step 5000 training loss 2.40279346963 training error: 0.16\n",
    "step 10000 training loss 2.32252722373 training error: 0.04\n",
    "step 15000 training loss 2.32432520667 training error: 0.06\n",
    "step 20000 training loss 2.32351511378 training error: 0.06\n",
    "step 25000 training loss 2.29420776586 training error: 0.04\n",
    "training loss 2.28214673939 training error: 0.0176666666667\n",
    "test loss 2.29084446693 test error: 0.027\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code, you will see the result above. We have a significant increase of accuracy from 84% to 98%, and the code runs faster. However, the result is far from the state-of-the-art (Nowadays, 1% error rate is very common), as this is only a tutorial and running a too large network consumes too much time and compute resources. If we increase the network size and keep training, we probably will get much better result. Besides, there are many more interesting techniques in deep learning, but they are out of this simple tutorial's scope.\n",
    "\n",
    "Finally, Hugo Larochelle hosts a great introductory course about feed-forward neural network and the training neural network on the website http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html. You can watch some videos if you want to learn more. Hope you have a sense of the deep learning now, and thank you all for reading."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
